# tf tools
import tensorflow as tf
# image processsing
from tensorflow.keras.preprocessing.image import (load_img,
                                                  img_to_array,
                                                  ImageDataGenerator)
# VGG16 model
from tensorflow.keras.applications.vgg16 import (preprocess_input,
                                                 decode_predictions,
                                                 VGG16)
# layers
from tensorflow.keras.layers import (Flatten, 
                                     Dense, 
                                     Dropout, 
                                     BatchNormalization)
# generic model object
from tensorflow.keras.models import Model
# optimizers
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import SGD
#scikit-learn
from sklearn.preprocessing import LabelBinarizer
from sklearn.metrics import classification_report
# for plotting
import numpy as np
import matplotlib.pyplot as plt
import os
# for json files
import pandas as pd


# plotting function
def plot_history(H, epochs):
    plt.style.use("seaborn-colorblind")

    plt.figure(figsize=(12,6))
    plt.subplot(1,2,1)
    plt.plot(np.arange(0, epochs), H.history["loss"], label="train_loss")
    plt.plot(np.arange(0, epochs), H.history["val_loss"], label="val_loss", linestyle=":")
    plt.title("Loss curve")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.tight_layout()
    plt.legend()

    plt.subplot(1,2,2)
    plt.plot(np.arange(0, epochs), H.history["accuracy"], label="train_acc")
    plt.plot(np.arange(0, epochs), H.history["val_accuracy"], label="val_acc", linestyle=":")
    plt.title("Accuracy curve")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.tight_layout()
    plt.legend()
    plt.show()

# Load the json metadata (containing the labels)
test_meta_path = os.path.join("..", "..", "431824", "images", "metadata","test_data.json" )
test_metadata = pd.read_json(test_meta_path, lines=True)
train_meta_path = os.path.join("..", "..", "431824", "images", "metadata","train_data.json" )
train_metadata = pd.read_json(train_meta_path, lines=True)
val_meta_path = os.path.join("..", "..", "431824", "images", "metadata","val_data.json" )
val_metadata = pd.read_json(val_meta_path, lines=True)

# creating random samples to make machine faster
test_metadata = test_metadata.sample(n=150)
train_metadata = train_metadata.sample(n=500)
val_metadata = val_metadata.sample(n=100)


# generators for splitting data into test, train and validation
train_generator = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
)
val_generator = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
)
test_generator = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
)

# Split the data and combining images and labels
directory_images = os.path.join("..", "..", "431824")
TARGET_size=(224, 224)
BATCH_Size = 32

train_images = train_generator.flow_from_dataframe(
    dataframe=train_metadata,
    directory = directory_images,
    x_col='image_path',
    y_col='class_label',
    target_size=TARGET_size,
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_Size,
    shuffle=True,
    seed=42,
    subset='training'
)

val_images = val_generator.flow_from_dataframe(
    dataframe=val_metadata,
    directory = directory_images, 
    x_col='image_path',
    y_col='class_label',
    target_size=TARGET_size,
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_Size,
    shuffle=True,
    seed=42,
)

test_images = test_generator.flow_from_dataframe(
    dataframe=test_metadata,
    directory = directory_images,
    x_col='image_path',
    y_col='class_label',
    target_size=TARGET_size,
    color_mode='rgb',
    class_mode='categorical',
    batch_size=BATCH_Size,
    shuffle=False
)

# load the VGG16 model without classifier layers
model = VGG16(include_top=False,
              pooling='avg', 
              input_shape=(224, 224, 3)) # this should fit the size of the indo images

# mark loaded layers as not trainable (freeze all weights)
for layer in model.layers: 
    layer.trainable = False

# add new classifier layers
flat1 = Flatten()(model.layers[-1].output) # flatten the images
bn = BatchNormalization()(flat1) # batch normalization layer
class1 = Dense(256, 
               activation='relu')(bn)
class2 = Dense(128, 
               activation='relu')(class1)
output = Dense(10, 
               activation='softmax')(class2)

# define new model
model = Model(inputs=model.inputs, 
              outputs=output)

# compile 
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.01,
    decay_steps=10000,
    decay_rate=0.9)
sgd = SGD(learning_rate=lr_schedule)

model.compile(optimizer=sgd,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# summarize model
model.summary()

# Binarize images?? (as in nb 9)

#fit model to indo fashion

history = model.fit(
    train_images, 
    steps_per_epoch = len(train_images),
    validation_data  = val_images,
    validation_steps = len(val_images),
    epochs = 10
    )

# train model on only a subset of data (because it takes several hours!)
